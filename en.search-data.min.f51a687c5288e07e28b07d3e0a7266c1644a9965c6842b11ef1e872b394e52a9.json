[{"id":0,"href":"/blog/","title":"Blog: C++, Optimization, and Low-Latency Trading Systems","section":"Welcome to AlgoMetix","content":" Welcome to the Algometix Blog # This blog is dedicated to in-depth technical discussions on C++ programming, system optimization, and low-latency development for high-performance trading applications.\nTopics Covered: # Advanced C++ Concepts: Templates, memory management, concurrency, and STL internals. Performance Optimization: Cache efficiency, branch prediction, SIMD optimizations. Low-Latency Trading Systems: Market data handling, order book design, tick-to-trade latency improvements. Algorithmic Trading Development: Execution strategies, risk management, and high-frequency trading infrastructure. Explore the categories in the sidebar to find relevant articles.\n"},{"id":1,"href":"/blog/cpu_cache/","title":"CPU Cache: Organization, Optimization and Challenges","section":"Blog: C++, Optimization, and Low-Latency Trading Systems","content":" CPU Cache : Organization, Optimization , Challenges, and Best Practices # Efficient CPU cache utilization is critical for achieving high-performance computing, especially in low-latency and high-throughput applications such as trading systems. This section of the blog explores:\nKey Topics Covered: # Cache Hierarchy \u0026amp; Organization: Understand L1, L2, and L3 caches, associativity, and their impact on performance. Multicore Cache Architectures: Learn how modern CPUs handle cache sharing, cache contention, and NUMA effects. False Sharing \u0026amp; Performance Bottlenecks: Identify and mitigate false sharing scenarios that degrade performance in multithreaded applications. Cache Coherence Protocols: Explore MESI, MOESI, and other protocols that maintain data consistency across multiple cores. Cache Optimization Techniques: Deep dive into data alignment, prefetching strategies, cache blocking, and ways to minimize cache misses. These blogs provide in-depth insights, hands-on examples, and benchmarking techniques to help developers optimize cache usage for real-world applications.\nWho Should Read This? # C++ and systems programmers looking to improve application performance. Low-latency developers working in trading systems and real-time applications. Performance engineers optimizing software for multi-core and NUMA architectures. Stay tuned for detailed technical articles, case studies, and hands-on optimization strategies.\n"},{"id":2,"href":"/blog/cpu_cache/cache-optimization/","title":"Cache Optimization Techniques","section":"CPU Cache: Organization, Optimization and Challenges","content":" Cache Optimization Techniques: Boost CPU Performance \u0026amp; Reduce Latency # Cache optimization plays a crucial role in boosting CPU performance, reducing latency, and improving overall system efficiency. In this section, we will explore advanced cache optimization strategies such as data alignment, prefetching, and more.\nKey Topics: # Data Alignment for Cache Efficiency Prefetching Techniques Minimizing Cache Misses Improving Data Locality Explore the articles and strategies in this section to enhance your understanding and implementation of cache optimization in real-time systems.\n"},{"id":3,"href":"/blog/cpu_cache/cache-optimization/cache-optimization-focusing-on-data-alignment/","title":"Data Alignment: Enhancing Contiguous Data Processing","section":"Cache Optimization Techniques","content":" Cache Optimization: Focusing on Data Alignment to Enhance Contiguous Data Processing. # Cache optimization is a cornerstone of high-performance computing. This article focuses on optimizing data alignment and data structures for improved cache utilization. By leveraging benchmarking results, we refine the design and implementation of a MarketData struct to illustrate tangible performance gains through alignment techniques.\nNote # This is the first article in a series on cache optimization, concentrating on alignment. To keep things simple, we omit multicore processing issues. In such scenarios, the shared nature of L1 and L2 caches introduces complexities like false sharing, fault tolerance, and more. These will be addressed in future articles. Techniques like buffering and prefetching, while omitted here, will also be covered in subsequent discussions.\nInitial Implementation: Baseline Design # The initial design of the MarketData struct is as follows:\nstruct MarketData { int symbol_id; // 4 bytes double price; // 8 bytes int volume; // 4 bytes }; Analysis # Field Sizes and Padding: int fields (symbol_id and volume) are 4 bytes each, while double price is 8 bytes. Due to alignment rules, the compiler adds padding after symbol_id (4 bytes) and after volume (4 bytes), making the total size of the struct 24 bytes (16 bytes for fields + 8 bytes padding). Cache Line Fit: Assuming a typical cache line size of 64 bytes, each instance of MarketData leaves unused space in the cache line. This could lead to inefficient cache utilization when processing arrays of MarketData. Letâ€™s address these inefficiencies through iterative improvements.\nImproving Memory Layout and Alignment # Reordering Fields to Minimize Padding # By rearranging fields, we can reduce the padding and optimize the memory layout.\nstruct MarketDataReordered { double price; // 8 bytes int symbol_id; // 4 bytes int volume; // 4 bytes }; Explanation # Field Sizes and Padding: Placing the largest field (double price) first eliminates the padding after symbol_id. Total size is now 16 bytes (all fields fit contiguously without padding). Cache Line Fit: This smaller size increases the number of MarketDataReordered instances that fit in a single cache line, improving cache efficiency during sequential access. Explicit Alignment for Cache Line Optimization # Aligning the struct to the cache line size ensures that each instance starts at a cache-line boundary, reducing cache contention in multithreaded scenarios.\nstruct alignas(64) MarketDataAligned { double price; // 8 bytes int symbol_id; // 4 bytes int volume; // 4 bytes // Padding: 48 bytes (added to make total size 64 bytes) }; Note # While aligning to 64 bytes is useful in multithreaded contexts to avoid false sharing, it introduces unnecessary memory overhead for single-threaded applications. For optimal single-threaded performance, the MarketDataReordered struct (16 bytes) is preferred. Further Improvement by Aligning Containers to Cache Line # Efficient batch processing requires that arrays or vectors of data are also cache-aligned.\nUsing Aligned Arrays # Aligned arrays ensure contiguous, cache-friendly storage for fixed-size data.\ntemplate \u0026lt;typename T, std::size_t N\u0026gt; struct AlignedArray { alignas(64) std::array\u0026lt;T, N\u0026gt; data; }; using AlignedMarketDataArray = AlignedArray\u0026lt;MarketDataReordered, 1000\u0026gt;; Explanation # Field Sizes and Padding: Each MarketDataReordered instance is 16 bytes. The total size of the array is a multiple of 64 bytes, ensuring cache alignment. Cache Line Fit: Sequential processing of AlignedMarketDataArray leverages cache lines effectively, reducing cache misses. C-Style Aligned Arrays # For applications requiring C-style arrays, similar alignment can be achieved using explicit memory alignment.\nstruct AlignedCArray { alignas(64) MarketDataReordered data[1000]; }; Using Aligned Vectors # Dynamic arrays can also benefit from cache alignment by using a custom aligned allocator.\ntemplate \u0026lt;typename T, std::size_t Alignment\u0026gt; struct aligned_allocator { using value_type = T; T* allocate(std::size_t n) { void* ptr = nullptr; if (posix_memalign(\u0026amp;ptr, Alignment, n * sizeof(T)) != 0) throw std::bad_alloc(); return static_cast\u0026lt;T*\u0026gt;(ptr); } void deallocate(T* ptr, std::size_t) { free(ptr); } }; using AlignedVector = std::vector\u0026lt;MarketDataReordered, aligned_allocator\u0026lt;MarketDataReordered, 64\u0026gt;\u0026gt;; Explanation # Dynamic Size Flexibility: While dynamic allocation adds overhead, the aligned allocator ensures that data is cache-aligned for efficient access. Additional Optimizations # Loop Unrolling # Unrolling loops reduces loop control overhead and enhances instruction-level parallelism. Additionally, it leverages cache efficiency by processing multiple elements loaded into a cache line in a single iteration.\nvoid Process(AlignedMarketDataArray updates) { #pragma GCC unroll 4 for (const auto\u0026amp; update : updates.data) { // Process market data } } Why Unroll 4? # Cache Line Fit: With a MarketDataReordered size of 16 bytes and a 64-byte cache line, four instances fit perfectly into a single cache line. Unrolling the loop by 4 ensures that all elements loaded into the cache line are processed in one iteration, maximizing cache utilization. Reduced Loop Overhead: Fewer loop control instructions are executed, improving efficiency. Other Benefits: Unrolling also allows better instruction pipelining and parallelism, enabling the CPU to execute multiple instructions simultaneously. Explanation # By unrolling the loop to match the number of elements fitting into a cache line, we align the processing logic with hardware-level optimizations. This reduces memory access latencies and maximizes throughput. Conclusion # Through iterative optimizations, we achieved:\nReduced padding by reordering fields. Aligned data structures to cache line boundaries (where necessary). Leveraged aligned containers for batch processing. Enhanced performance with loop unrolling tailored to cache line size. Future work will explore techniques like buffering and prefetching, along with advanced considerations for multicore architectures, in subsequent articles.\n"},{"id":4,"href":"/blog/cpu_cache/cache-optimization/cache-optimization-through-prefetching/","title":"Cache Prefetching: Enhancing Non-Contiguous Data Processing","section":"Cache Optimization Techniques","content":" Cache Optimization through Prefetching: Enhancing Non-Contiguous Data Processing # In the realm of high-performance computing, cache optimization is crucial for ensuring efficient data processing. My previous article focused on optimizing cache usage through data alignment, where we rearranged data structures and leveraged container alignment to maximize cache utilization. This technique works well for data stored in contiguous memory. However, in real-world scenarios, data might not always be stored in contiguous blocks. This article focuses on how to handle or optimize such data for cache using prefetching techniques.\nWhat is Prefetching? # Prefetching is a technique that tells the CPU to load data into the cache before it is needed. By doing this, we can reduce the time the CPU spends waiting for data from memory, which can significantly speed up processing.\nWhy Use Prefetching? # When data is not stored contiguously (e.g., linked lists), accessing it can cause cache misses. Prefetching helps avoid these misses by bringing the required data into the cache in advance.\nExample: Order Processing with Linked List # Letâ€™s consider a simple example of processing Order data stored in a linked list:\nInitial Structure and Processing # struct Order { double price; int orderID; int quantity; }; Processing orders in a linked list without prefetching:\nvoid processOrdersWithoutPrefetching(std::list\u0026lt;Order\u0026gt;\u0026amp; orders) { for (const auto\u0026amp; order : orders) { processOrder(order); } } This straightforward approach may suffer from cache misses because linked lists store their elements in non-contiguous memory.\nIntroducing Prefetching # Prefetching instructs the CPU to load data into the cache before it is accessed, reducing memory access latency. Modern CPUs support prefetching with non-blocking instructions, allowing subsequent instructions to execute while prefetching occurs in parallel. This non-blocking nature ensures that the CPU can continue executing other instructions while the data is being fetched into the cache.\nEnhancing the above code with prefetching:\nvoid processOrdersWithPrefetching(std::list\u0026lt;Order\u0026gt;\u0026amp; orders) { auto it = orders.begin(); // Prefetch the first element if (it != orders.end()) { __builtin_prefetch(\u0026amp;(*it)); } for (; it != orders.end(); ++it) { auto nextIt = std::next(it); if (nextIt != orders.end()) { __builtin_prefetch(\u0026amp;(*nextIt)); } processOrder(*it); } } Note: In this article, we provide a general overview of prefetching and use the default parameters for __builtin_prefetch. Detailed explanations of the parameters and their tuning are beyond the scope of this article.\nKey Points About Prefetching # Non-Blocking Nature: # Prefetching does not stop the CPU from executing other instructions. While the data is being fetched, the CPU continues to process. Optimal Distance: # The CPU needs time to fetch the prefetched data into the cache. Prefetching too close to the current access point may not give enough time, causing delays. Prefetching too far ahead may waste resources. ####### Step-by-Step Example 1. First Prefetch*: - At the start, we prefetch the first order before we enter the loop. - This ensures the first piece of data is ready to use right away.\nauto it = orders.begin(); if (it != orders.end()) { __builtin_prefetch(\u0026amp;(*it)); } 2. **Inside the Loop**: - Inside the loop, we keep prefetching the next order while processing the current one. - This keeps the next piece of data ready in advance. for (; it != orders.end(); ++it) { auto nextIt = std::next(it); if (nextIt != orders.end()) { __builtin_prefetch(\u0026amp;(*it)); } processOrder(*it); } 3. **Platform-Specific Instructions**: - `_mm_prefetch` works for Intel platforms. For other systems, check compiler-specific options, such as GCCâ€™s `__builtin_prefetch`. Improved Prefetching with Batch Processing # Batch processing involves fetching and processing multiple data items together to reduce cache misses and improve performance.\nWhy Batch Processing? # Cache Line Utilization: Modern CPUs fetch data in blocks (cache lines). Using data in batches ensures efficient use of these blocks. Example Code for Batch Prefetching # Letâ€™s fetch and process orders in batches of 4:\nvoid processOrdersInBatches(std::list\u0026lt;Order\u0026gt;\u0026amp; orders) { auto it = orders.begin(); Order* orderBatch[4]; while (it != orders.end()) { size_t batchSize = 0; // Prefetch the next 4 orders and collect them in a batch for (int i = 0; i \u0026lt; 4 \u0026amp;\u0026amp; it != orders.end(); ++i, ++it) { __builtin_prefetch(\u0026amp;(*it)); orderBatch[batchSize++] = \u0026amp;(*it); } // Process the batch of prefetched orders processOrderBatch(orderBatch, batchSize); } } Key Steps in Batch Processing # ####### Why Size 4?\nCache Line Size: A typical cache line is 64 bytes. Order Size: Each Order struct is 16 bytes (after reordering). Fit 4 Orders: Since ( 64 , \\text{bytes} \\div 16 , \\text{bytes/order} = 4 ), we can fit 4 Order structs in one cache line. ####### Step-by-Step Example\n1. **Initialize Batch and Iterator**: - We start by initializing an iterator and a batch array to hold 4 orders. auto it = orders.begin(); Order* orderBatch[4]; 2. **Prefetch and Process in Batches**: - Inside the loop, we prefetch the next 4 orders and store them in the batch array. - This ensures that the next 4 orders are fetched into the cache before they are processed. while (it != orders.end()) { size_t batchSize = 0; // Prefetch the next 4 orders and collect them in a batch for (int i = 0; i \u0026lt; 4 \u0026amp;\u0026amp; it != orders.end(); ++i, ++it) { __builtin_prefetch(\u0026amp;(*it)); orderBatch[batchSize++] = \u0026amp;(*it); } // Process the batch of prefetched orders processOrderBatch(orderBatch, batchSize); } 3. **Prefetch 4 Orders**: - The loop runs 4 times (if there are 4 orders available). - Each iteration prefetches one order and adds it to the batch array. 4. **Process the Batch**: - After collecting 4 orders (or fewer, if fewer are left), we process them in the batch. - This ensures that the fetched orders are used efficiently. Google Benchmark: # Run on (4 X 3300 MHz CPU s) CPU Caches: L1 Data 32 KiB (x2) L1 Instruction 32 KiB (x2) L2 Unified 256 KiB (x2) L3 Unified 3072 KiB (x1) Load Average: 3.07, 2.56, 1.98 ***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead. ----------------------------------------------------------------------------- Benchmark Time CPU Iterations ----------------------------------------------------------------------------- BM_ProcessOrdersWithoutPrefetching 5556028 ns 5548987 ns 254 BM_ProcessOrdersWithPrefetching 5335091 ns 5329073 ns 263 BM_ProcessOrdersWithPrefetchBatch 4569271 ns 4563944 ns 312 Explanation: # Compiler and Flags: The benchmark was built using GCC 13 with -O3 optimization. Observations: Prefetching showed a modest improvement over the baseline (WithoutPrefetching). The difference between WithoutPrefetching and WithPrefetching was small, likely due to compiler optimizations and automatic vectorization. Batch prefetching demonstrated the most significant performance gain due to efficient cache utilization. Conclusion # Prefetching is a powerful tool for optimizing cache usage, especially for non-contiguous data structures like linked lists. By understanding the non-blocking nature of prefetch instructions, combining them with batch processing, and tailoring them to specific hardware, you can significantly reduce cache misses and improve performance. This article complements the earlier discussion on data alignment, offering a comprehensive approach to cache optimization in diverse scenarios.\n"},{"id":5,"href":"/blog/cpu_cache/multicore-caching/","title":"Multi-Core Caching Techniques","section":"CPU Cache: Organization, Optimization and Challenges","content":" Multi-Core Architecture \u0026amp; CPU Caching: Performance \u0026amp; Optimization # Multi-core systems require specialized cache management strategies to maximize performance. In this section, we cover topics like cache coherence, false sharing, and how multi-core architecture impacts CPU caching.\nKey Topics: # Cache Coherence Protocols Cache Hierarchy and Sharing False Sharing in Multi-Core Systems Parallel Computing Optimization Explore the articles in this section to gain a deeper understanding of multi-core CPU caching and performance tuning.\n"},{"id":6,"href":"/blog/cpu_cache/multicore-caching/cache-hierarchy/","title":"Cache Hierarchy and Sharing","section":"Multi-Core Caching Techniques","content":" Cache Hierarchy and Sharing in Multi-Processor Machines # Typical Cache Hierarchy # In multi-core processors, the cache hierarchy plays a crucial role in improving performance by minimizing the time taken to access frequently used data. Here\u0026rsquo;s a breakdown of the typical cache levels:\nL1 Cache: The smallest and fastest cache, private to each core. L2 Cache: Larger than L1, usually private to each core but can also be shared in some systems. L3 Cache: The largest cache, shared across all cores in the processor. Main Memory (RAM): The largest and slowest storage layer, accessed when data is not found in the caches. Single-Core vs Multi-Core Architecture # We moved from single-core to multi-core processors, which made computers faster, especially by running tasks in parallel. However, this also increased complexity for developers, requiring careful optimization for efficient code execution. To understand this better, letâ€™s examine the basic structure of multi-core processors and how they handle memory and cache.\nThe following diagram illustrates the difference between a single-core processor and a multi-core processor, highlighting the relationship between CPU cores, their respective caches, and main memory.\nUnderstanding the Diagram # The diagram shows two types of processors:\nSingle-Core Processor\nHas one core handling all tasks. Includes private L1 and L2 caches to store frequently used data. Accesses main memory (RAM) when data is not available in the cache. Multi-Core Processor\nHas multiple cores, allowing parallel execution. Each core has its own private L1 and L2 caches (as commonly seen in Intel processors). An L3 cache acts as a buffer for data exchange between cores and helps reduce main memory accesses. Reduces the need to access main memory, improving performance. Why This Matters # Multi-core processors improve speed and efficiency, but they also introduce new challenges like cache coherence, false sharing, and synchronization overhead. The rest of the article will explore these issues and their impact on performance.\nChecking Your CPU\u0026rsquo;s Cache Configuration # If you have an Intel processor like mine, you can check your systemâ€™s cache configuration using the following command:\n$ lscpu | grep -i cache L1d cache: 64 KiB (2 instances) L1i cache: 64 KiB (2 instances) L2 cache: 512 KiB (2 instances) L3 cache: 3 MiB (1 instance) Note: This configuration is specific to my Intel processor and may differ for other models.\nInterpretation # L1d \u0026amp; L1i caches: 64 KiB each with 2 instances â†’ This indicates that L1 cache is private per core. L2 cache: 512 KiB with 2 instances â†’ Each core likely has its own private L2 cache. L3 cache: 3 MiB with 1 instance â†’ This shows that L3 cache is shared across all cores. This system has private L1 and L2 caches per core and a shared L3 cache, which is a common configuration in modern processors like those from Intel.\nCache Sharing and Potential Issues # Private Caches: Cores have their own L1 and L2 caches, designed to provide fast access to frequently used data specific to each core. Shared Cache: The L3 cache is shared by all cores, allowing for efficient communication and data exchange between cores. However, cache sharing introduces some challenges:\nIssues in Multi-Core Architectures # Cache Coherence: To ensure all cores have a consistent view of the data, cache coherence protocols (like MESI) are employed. These protocols track the state of each cache line to maintain consistency.\nFalse Sharing: False sharing occurs when independent variables, stored in the same cache line, are modified by different threads running on separate cores. This results in unnecessary cache invalidation, reducing performance.\nCache Thrashing: When multiple cores frequently access and modify the same cache line, it leads to constant invalidation and reloading, significantly impacting performance.\n"},{"id":7,"href":"/blog/cpu_cache/multicore-caching/false-sharing/","title":"False Sharing","section":"Multi-Core Caching Techniques","content":" What is False Sharing? # False sharing occurs when multiple threads modify independent variables that reside in the same cache line, leading to unnecessary cache invalidation and performance degradation. This issue is often subtle and may not be immediately apparent in code, but it can significantly impact performance in multi-threaded programs.\nWhy is it Called \u0026lsquo;False\u0026rsquo; Sharing? # What is shared? A cache line containing different variables used by multiple threads. Why \u0026lsquo;false\u0026rsquo;? The variables are not logically shared in terms of their use by different threads, but because they occupy the same cache line, any modification by one thread forces an update across multiple cores, causing inefficient cache invalidations and increased cache coherence traffic. When Does False Sharing Happen? # False sharing occurs in multi-threaded programs due to the interaction between the following factors:\nAdjacent variables: When threads modify independent variables stored adjacently in memory, they may end up in the same cache line. Cache coherence protocols: In systems with cache coherence protocols (e.g., MESI), even though each core has its own private cache (L1/L2), any modification in one core\u0026rsquo;s cache must be reflected across all cores to maintain consistency. This forces unnecessary cache invalidations and cache misses when a variable in the same cache line is modified by different threads. Private L1/L2 caches: Although caches are private to each core, the cache coherence protocol ensures consistency across caches, triggering updates or invalidations in other coresâ€™ caches, which can degrade performance due to false sharing. This combination of adjacent data in the same cache line and the behavior of cache coherence protocols results in costly synchronization and performance degradation, even when the variables are independent.\n"},{"id":8,"href":"/blog/cpu_cache/multicore-caching/false-sharing/false_sharing_adjacent_variables/","title":"Impact of Adjacent Variable Modification","section":"False Sharing","content":" Impact of Adjacent Variable Modification by Multiple Threads # False sharing occurs when multiple threads modify adjacent data stored in memory, leading to performance degradation due to unnecessary cache invalidations. In modern multi-core systems, where cache coherence protocols are crucial, minimizing false sharing becomes critical for optimizing performance in multithreaded applications.\nCache Line Contention: Visual Representation # Diagram for False Sharing in Adjacent Data # In this diagram, Variable A and Variable B are stored in the same cache line, causing both cores to continuously invalidate each other\u0026rsquo;s cache when either variable is updated.\nCode Examples: False Sharing in Independent and Struct Variables # 1. False Sharing with Independent Variables # #include \u0026lt;iostream\u0026gt; #include \u0026lt;thread\u0026gt; const int NUM_ITER = 10000000; int a = 0; // Modified by Thread 1 int b = 0; // Modified by Thread 2 void threadFunc1() { for (int i = 0; i \u0026lt; NUM_ITER; ++i) { a++; } } void threadFunc2() { for (int i = 0; i \u0026lt; NUM_ITER; ++i) { b++; } } int main() { std::thread t1(threadFunc1); std::thread t2(threadFunc2); t1.join(); t2.join(); std::cout \u0026lt;\u0026lt; \u0026#34;Final values: \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; b \u0026lt;\u0026lt; std::endl; } In this example, a and b are adjacent in memory, possibly on the same cache line. As thread 1 modifies a and thread 2 modifies b, the cache line containing both variables must be invalidated and reloaded, causing excessive cache coherence traffic and negatively affecting performance.\n2. False Sharing with Struct Variables # #include \u0026lt;iostream\u0026gt; #include \u0026lt;thread\u0026gt; const int NUM_THREADS = 2; const int NUM_ITER = 10000000; struct SharedData { int a; // Modified by Thread 1 int b; // Modified by Thread 2 } data; void threadFunc1() { for (int i = 0; i \u0026lt; NUM_ITER; ++i) { data.a++; // This will cause false sharing with data.b } } void threadFunc2() { for (int i = 0; i \u0026lt; NUM_ITER; ++i) { data.b++; // This will cause false sharing with data.a } } int main() { std::thread t1(threadFunc1); std::thread t2(threadFunc2); t1.join(); t2.join(); std::cout \u0026lt;\u0026lt; \u0026#34;Final values: \u0026#34; \u0026lt;\u0026lt; data.a \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; data.b \u0026lt;\u0026lt; std::endl; } Similarly, in this example, data.a and data.b are adjacent in memory, possibly on the same cache line. As thread 1 modifies data.a and thread 2 modifies data.b, the cache line containing both member variables must be invalidated and reloaded, causing excessive cache coherence traffic and negatively affecting performance.\nOptimizing for Performance: Mitigating False Sharing # 1. Use Alignment to Separate Variables # a. Stack or Global Variables with alignas # alignas(64) int a = 0; alignas(64) int b = 0; b. Heap Allocation with Alignment # int* a = static_cast\u0026lt;int*\u0026gt;(std::aligned_alloc(64, 64)); int* b = static_cast\u0026lt;int*\u0026gt;(std::aligned_alloc(64, 64)); Note: std::aligned_alloc is available only in C++17 and later versions. If working with an earlier C++ version, consider using posix_memalign or similar alternatives for heap alignment.\nPrevents automatic adjacent placement in memory.*\n2. Use Padding to Separate Variables # struct PaddedInt { int value; char padding[60]; // Assuming a 64-byte cache line }; PaddedInt a; PaddedInt b; struct alignas(64) SharedData { int a; char padding[60]; // Padding to ensure a and b do not share the same cache line int b; }; Forces a and b to be allocated in different cache lines, reducing contention.\n3. Using Thread-Local Storage (thread_local) # thread_local int a; thread_local int b; Using thread_local ensures that each thread gets its own instance of a and b, stored in thread-local storage, which prevents false sharing as the variables are not shared between threads.\nKey Takeaways # False sharing occurs when multiple threads modify adjacent variables that share the same cache line, leading to performance degradation. The problem is the same whether the variables are independent or part of a struct. Mitigation strategies include alignment, padding, splitting variables, and using thread-local storage. "},{"id":9,"href":"/blog/cpu_cache/multicore-caching/false-sharing/partitioned_array_processing_and_false_sharing/","title":"Impact of Adjacent Array Indices Modification","section":"False Sharing","content":" Scenario: Impact of Adjacent Element Modification During Parallel Loop Processing # Threads process separate parts of an array. When adjacent array elements reside within the same cache line, modifications by different threads can lead to performance degradation. This is because even though threads are working on logically distinct elements, cache line invalidation forces threads to reload data unnecessarily. False Sharing Illustration # Code: Parallel Order Book Modification (Illustrating Potential False Sharing) # Introduction to Order Books and Price Level Orders # In financial markets, an order book aggregates buy and sell orders for an asset. Each order specifies:\nPrice: The price at which the order is placed. Volume: The amount of the asset to be bought or sold at that price. Orders are grouped by price level, representing the available buy and sell interest at different price points.\nCode Breakdown # #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;thread\u0026gt; #include \u0026lt;atomic\u0026gt; #include \u0026lt;chrono\u0026gt; const int NUM_THREADS = 4; // Number of threads to use // Structure for an order book level struct OrderBookLevel { int price; // Price of the order int volume; // Volume of the order }; std::vector\u0026lt;OrderBookLevel\u0026gt; order_book; // Global vector holding the order book // Function to modify orders in the order book void modify_orders(int thread_id) { int start_index = thread_id * (order_book.size() / NUM_THREADS); // Starting index for this thread\u0026#39;s partition int end_index = start_index + (order_book.size() / NUM_THREADS); // Ending index for this thread\u0026#39;s partition for (int i = start_index; i \u0026lt; end_index; ++i) { // Simulate order cancellation/modification if (order_book[i].volume \u0026gt; 100) { order_book[i].volume -= 100; // Reduce volume (partial cancellation) } } } int main() { std::vector\u0026lt;std::thread\u0026gt; threads; // Vector to hold threads // Create and start threads for (int i = 0; i \u0026lt; NUM_THREADS; ++i) { threads.emplace_back(modify_orders, i); // Launch threads to modify orders in parallel } // Wait for threads to complete for (auto\u0026amp; t : threads) { t.join(); } return 0; } Explanation: # OrderBookLevel Structure: Represents a single order at a given price level, containing its price and volume. order_book (Global): A vector holding all orders, with each entry representing a specific price level. Parallel Modification: The code uses multi-threading to parallelize order book modifications. Each thread processes a portion of the order_book, reducing order volume if it exceeds 100. modify_orders Function: Each thread works on a segment of the order book defined by start_index and end_index. It simulates order modification by decreasing the volume (if greater than 100). Thread Management: Threads are created and joined to ensure parallel execution and completion. Potential False Sharing # False sharing occurs when multiple threads modify logically independent data that happens to reside within the same cache line. This leads to unnecessary cache invalidations and reloads, significantly degrading performance.\nIn this code:\nThe order_book vector is shared, and each thread modifies different OrderBookLevel objects. If adjacent OrderBookLevel objects happen to fall on the same cache line, modifying them from different threads can trigger false sharing. For example, if OrderBookLevel[i].price and OrderBookLevel[i+1].volume reside on the same cache line, modifying them concurrently will cause cache invalidations, even though the data is logically distinct.\nMitigation Strategies for False Sharing (Ordered by Potential Efficiency) # 1. SoA: Separate Arrays (Optionally Grouped in a Struct) # Instead of using an Array of Structures (AoS), the key idea is to store each field (price, volume, etc.) in separate arrays. This organization reduces false sharing and can improve memory access patterns.\nOption A: Separate Arrays Only\nThis is the simplest implementation, where you declare separate arrays for each field:\n// Separate arrays for prices and volumes // std::vector\u0026lt;int\u0026gt; prices(ORDER_BOOK_SIZE); //Uncomment this line in main as well // std::vector\u0026lt;int\u0026gt; volumes(ORDER_BOOK_SIZE); //Uncomment this line in main as well Option B: Separate Arrays Grouped in a Struct\nFor better organization, you can group the separate arrays within a struct:\n// Structure to hold the separate arrays struct OrderBook { std::vector\u0026lt;int\u0026gt; prices; std::vector\u0026lt;int\u0026gt; volumes; }; Advantages and Disadvantages (Concise):\nOrganization: Option B (struct) provides better logical grouping of related arrays. Access: Option A (separate arrays) uses direct access; Option B (struct) accesses via struct members. Complete Example Code (Option B - Separate Arrays in a Struct):\nThe following code demonstrates how to modify volumes using Option B and addresses the issues mentioned:\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;thread\u0026gt; const int NUM_THREADS = 4; const int ORDER_BOOK_SIZE = 1000; // Structure to hold the separate arrays struct OrderBook { std::vector\u0026lt;int\u0026gt; prices; std::vector\u0026lt;int\u0026gt; volumes; }; // Don\u0026#39;t forget the semicolon! int main() { OrderBook order_book; //declare the struct so that its member can be accessed. order_book.prices.resize(ORDER_BOOK_SIZE); //initialize struct and its member order_book.volumes.resize(ORDER_BOOK_SIZE); // Initialize the data (example) for (int i = 0; i \u0026lt; ORDER_BOOK_SIZE; ++i) { order_book.prices[i] = 100 + i; order_book.volumes[i] = 500 + (i % 10) * 10; } // Create and launch threads std::vector\u0026lt;std::thread\u0026gt; threads; for (int i = 0; i \u0026lt; NUM_THREADS; ++i) { threads.emplace_back([\u0026amp;, i]() { int start_index = i * (order_book.volumes.size() / NUM_THREADS); int end_index = start_index + (order_book.volumes.size() / NUM_THREADS); for (int j = start_index; j \u0026lt; end_index; ++j) { //Correct the loop counter if (order_book.volumes[j] \u0026gt; 100) { order_book.volumes[j] -= 100; } } }); } // Join the threads for (auto\u0026amp; t : threads) { t.join(); } std::cout \u0026lt;\u0026lt; \u0026#34;Volumes modified successfully!\u0026#34; \u0026lt;\u0026lt; std::endl; return 0; } 2. Data Alignment with Padding # Use alignas(64) to enforce cache-line alignment and add padding within the structure to prevent adjacent elements from sharing a cache line.\nconst int CACHE_LINE_SIZE = 64; struct alignas(CACHE_LINE_SIZE) OrderBookLevel { int price; int volume; char padding[CACHE_LINE_SIZE - sizeof(int) * 2]; // Padding to fill the cache line }; Explanation:\nalignas(CACHE_LINE_SIZE): Ensures the OrderBookLevel struct starts at a cache line boundary. padding Member: Guarantees that each OrderBookLevel occupies a full cache line, preventing overlap and false sharing. Usage: For single instances of the struct, use both alignas and padding. For arrays, alignas applied to the array\u0026rsquo;s element type is generally sufficient. Benefits: Improves memory access patterns, reduces cache invalidation, and enhances performance in multi-threaded applications. 3. Partitioning with Padding # Divide the order_book into partitions, adding padding after each partition to ensure separation on cache lines. This strategy leverages std::span for efficient access to the partitions. The goal is to isolate the data accessed by each thread onto its own cache lines, preventing false sharing.\nCode:\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;thread\u0026gt; #include \u0026lt;span\u0026gt; #include \u0026lt;cstdint\u0026gt; const int NUM_THREADS = 4; // Number of threads const int ORDER_BOOK_SIZE = 1000; // Number of actual orders const int CACHE_LINE_SIZE = 64; // Cache line size in bytes const int ELEMENT_SIZE = sizeof(OrderBookLevel); // Size of OrderBookLevel struct const int PADDING_ELEMENTS = CACHE_LINE_SIZE / ELEMENT_SIZE; // Padding elements // Structure representing an order book level struct OrderBookLevel { int price; int volume; }; // Global order book with padding after each partition std::vector\u0026lt;OrderBookLevel\u0026gt; order_book(ORDER_BOOK_SIZE + (NUM_THREADS * PADDING_ELEMENTS)); // Function to modify orders using std::span, operating on partitioned data void modify_orders(std::span\u0026lt;OrderBookLevel\u0026gt; partition) { for (auto\u0026amp; level : partition) { if (level.volume \u0026gt; 100) { level.volume -= 100; } } } int main() { // Initialize order book with example data for (int i = 0; i \u0026lt; ORDER_BOOK_SIZE; ++i) { order_book[i].price = 100 + i; order_book[i].volume = 500 + (i % 10) * 10; } std::vector\u0026lt;std::thread\u0026gt; threads; int partition_size = ORDER_BOOK_SIZE / NUM_THREADS; for (int i = 0; i \u0026lt; NUM_THREADS; ++i) { int start_index = i * (partition_size + PADDING_ELEMENTS); // Include padding int end_index = start_index + partition_size; // Exclude padding // Create a span covering only the partition, NOT the padding std::span\u0026lt;OrderBookLevel\u0026gt; partition(order_book.data() + start_index, partition_size); threads.emplace_back(modify_orders, partition); } for (auto\u0026amp; t : threads) { t.join(); } std::cout \u0026lt;\u0026lt; \u0026#34;Order book modified successfully.\u0026#34; \u0026lt;\u0026lt; std::endl; return 0; } Walkthrough of Partitioning Logic: # Goal: Divide the order_book into NUM_THREADS partitions. Each thread will work on its own partition. We want to ensure that the data each thread works on resides on separate cache lines to avoid false sharing. Constants:\nORDER_BOOK_SIZE: The number of actual OrderBookLevel elements we want to store. CACHE_LINE_SIZE: The size of a cache line on the target architecture (e.g., 64 bytes). ELEMENT_SIZE: The size of the OrderBookLevel struct in bytes. PADDING_ELEMENTS: The number of OrderBookLevel elements needed to fill a full cache line. PADDING_ELEMENTS Calculation: This is the most important calculation:\nconst int PADDING_ELEMENTS = CACHE_LINE_SIZE / ELEMENT_SIZE; This tells us how many extra OrderBookLevel elements we need to add as padding after each partition to ensure that the next partition starts on a new cache line. If a cache line is 64 bytes, and each OrderBookLevel element is 8 bytes (2 ints), then PADDING_ELEMENTS will be 64 / 8 = 8. So we will insert eight padding elements at the end of each partition. order_book Size:\nThe total size of the order_book vector includes the actual order elements plus the padding:\nstd::vector\u0026lt;OrderBookLevel\u0026gt; order_book(ORDER_BOOK_SIZE + (NUM_THREADS * PADDING_ELEMENTS)); So the total size is ORDER_BOOK_SIZE plus PADDING_ELEMENTS for each thread.\nPartition size:\nEach thread gets the same number of elements to work on this is achieved through the use of this line.\nint partition_size = ORDER_BOOK_SIZE / NUM_THREADS; start_index Calculation:\nThe start_index determines where each thread\u0026rsquo;s partition begins within the order_book. This calculation includes the padding from previous partitions:\nint start_index = i * (partition_size + PADDING_ELEMENTS); For example: If you have three threads; the start_index of the partition would be the number of elements from previous partion added with padding from previous parition. If each partition is assigned partion_size elements and padded with PADDING_ELEMENTS this is the total that is mulitplied the thread number to assigned the start_index. std::span Construction:\nThe std::span provides a view of the data within the order_book that a thread is allowed to access. Crucially, the std::span only covers the actual order elements within the partition, excluding the padding:\nstd::span\u0026lt;OrderBookLevel\u0026gt; partition(order_book.data() + start_index, partition_size); The span starts at the correct start_index, but its length is only partition_size (the number of actual order elements). The span prevents the thread from accidentally accessing or modifying the padding. Iteration:\nThe modify_orders function uses a std::span to iterate only though a partion which is the order_book which is allocated to the current thread.\nfor (auto\u0026amp; level : partition) { if (level.volume \u0026gt; 100) { level.volume -= 100; } } Example Calculation:\nLet\u0026rsquo;s say:\nNUM_THREADS = 4 ORDER_BOOK_SIZE = 1000 CACHE_LINE_SIZE = 64 bytes sizeof(OrderBookLevel) = 8 bytes (2 ints) Then:\nPADDING_ELEMENTS = 64 / 8 = 8 The total size of order_book is 1000 + (4 * 8) = 1032 partition_size = 1000 / 4 = 250 For Thread 1 (i = 1):\nstart_index = 1 * (250 + 8) = 258 The std::span for Thread 1 will start at index 258 and have a length of 250. Thread 1 can access element from order_book[258] to order_book[507] without any risks. This detailed explanation and calculation example should make the partitioning logic much clearer to your readers.\n4. Local Buffers and Merging Results # Each thread processes data in its local buffer and then merges it back into the shared data after completion. This strategy eliminates false sharing but introduces a copying overhead.\nCode:\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;thread\u0026gt; #include \u0026lt;atomic\u0026gt; #include \u0026lt;chrono\u0026gt; const int NUM_THREADS = 4; // Number of threads const int ORDER_BOOK_SIZE = 1000; // Size of the order book // Structure for representing an order book level struct OrderBookLevel { int price; int volume; }; // Global vector to hold the order book std::vector\u0026lt;OrderBookLevel\u0026gt; order_book(ORDER_BOOK_SIZE); // Function to modify orders in a local buffer, then merge them back void modify_orders(int thread_id) { // Local buffer for the thread std::vector\u0026lt;OrderBookLevel\u0026gt; local_buffer(ORDER_BOOK_SIZE / NUM_THREADS); // Determine which portion of the order book the thread is responsible for int start_index = thread_id * (ORDER_BOOK_SIZE / NUM_THREADS); // Starting index int end_index = start_index + (ORDER_BOOK_SIZE / NUM_THREADS); // Ending index // Each thread processes its local buffer (work is done on the local copy) for (int i = start_index; i \u0026lt; end_index; ++i) { local_buffer[i - start_index] = order_book[i]; // Copy data to local buffer // Simulate order cancellation/modification if (local_buffer[i - start_index].volume \u0026gt; 100) { local_buffer[i - start_index].volume -= 100; } } // After local processing, merge results back into the main order book for (int i = start_index; i \u0026lt; end_index; ++i) { order_book[i] = local_buffer[i - start_index]; // Copy modified data back } } int main() { // Initialize order book for (int i = 0; i \u0026lt; ORDER_BOOK_SIZE; ++i) { order_book[i].price = 100 + i; order_book[i].volume = 500 + (i % 10) * 10; } std::vector\u0026lt;std::thread\u0026gt; threads; // Create and start threads for (int i = 0; i \u0026lt; NUM_THREADS; ++i) { threads.emplace_back(modify_orders, i); } // Wait for threads to complete for (auto\u0026amp; t : threads) { t.join(); } std::cout \u0026lt;\u0026lt; \u0026#34;Order book modified successfully.\u0026#34; \u0026lt;\u0026lt; std::endl; return 0; } Explanation and Considerations:\nLocal Buffer: Each thread operates on a local buffer, thus avoiding direct modifications to the global order_book. Merging: After processing, the thread merges back the data by merging it in main order_book Benefit: Working on local copies prevent false sharing. Limitations: This technique involves extra memory copy, which may outweigh the advantage of using multiple cores as the threads have to process the volume on their local copy. Also we cannot be sure whether this solution provides better efficiency than performing on an array on single process. It depends whether copying the data is more cheaper that overhead caused by cache invalidation due to false sharing. When to Consider Local Buffers:\nComplex Processing: If the processing within the loop is very complex and computationally intensive, the overhead of copying might be small compared to the overall processing time. High Contention: When there\u0026rsquo;s very high contention (frequent cache invalidation) due to false sharing, the local buffer approach might be beneficial. Important Note:\nIt\u0026rsquo;s crucial to benchmark and profile your code to determine whether this approach is actually improving performance compared to other techniques or even a single-threaded solution. The optimal strategy depends on the specific workload and hardware characteristics.\n"},{"id":10,"href":"/about/","title":"About","section":"Welcome to AlgoMetix","content":" About AlgoMetix # Who Am I? # Welcome to AlgoMetix! I am a seasoned C++ developer\u0026hellip;\nAbout AlgoMetix # Who Am I? # Welcome to AlgoMetix! I am a seasoned C++ developer with over 20 years of experience in low-latency and algorithmic trading platforms. Having worked in front-office trading desks at leading investment banks like Credit Suisse and BNP Paribas, I have deep expertise in designing and optimizing real-world trading systems. My focus has always been on performance, efficiency, and robustnessâ€”critical aspects of any high-performance computing application.\nWhat is AlgoMetix? # AlgoMetix is inspired by the Sanskrit word Tattvartha, meaning \u0026ldquo;true essence of things.\u0026rdquo; This aligns with the websiteâ€™s goal of exploring the fundamental principles behind high-performance computing, advanced C++ techniques, and real-world optimizations. The name reflects the focus on deep, practical insights rather than surface-level knowledge.\nWhat This Website Offers # This website is a place where I share my knowledge and experience in advanced C++ programming, low-latency system design, performance optimization, and real-world algorithmic trading architectures. My articles go beyond the standard programming books, covering:\nAdvanced C++ Techniques â€“ Templates, memory management, multi-threading, lock-free data structures, and more. Low-Latency Optimization â€“ CPU cache efficiency, false sharing, NUMA-aware programming, and compiler optimizations. Algorithmic Trading System Design â€“ Order execution, market data handling, risk management, and connectivity with exchanges. Efficient and Optimized Code â€“ Practical solutions that work in real-world production environments. Under-the-Hood Insights â€“ How compilers, memory models, and CPU architectures impact software performance. Why AlgoMetix? # The internet is full of beginner-friendly programming tutorials, but real-world high-performance and low-latency system design is rarely covered in depth. My goal is to fill that gap by providing practical, production-grade insights that are immediately useful for professionals working in trading, high-performance computing, and systems programming.\nConsulting Services # With my extensive industry experience, I am also available for consulting on:\nPerformance tuning of existing C++ codebases. Designing and optimizing low-latency trading systems. Architecting scalable, high-performance applications. Code reviews, mentoring, and training teams on advanced C++ topics. Get in Touch # If you are interested in consulting, collaborations, or have any questions, feel free to contact me at ppatoria.jp@gmail.com. I am always open to discussions on high-performance programming, optimization, and real-world system design.\nThank you for visiting AlgoMetix. I hope you find the content valuable and insightful!\n"},{"id":11,"href":"/consulting/","title":"Consulting: High-Performance Trading System Development","section":"Welcome to AlgoMetix","content":" Consulting Services # I offer specialized consulting services in the domain of high-performance trading system development. With extensive experience in low-latency programming and real-time financial applications, I provide:\nSystem Architecture \u0026amp; Development: Design and build scalable, high-throughput trading platforms from scratch. Performance Optimization: Analyze and optimize C++ code for latency reduction, CPU efficiency, and cache performance. Code Reviews \u0026amp; Best Practices: Conduct deep technical reviews to ensure adherence to best practices in C++, concurrency, and high-performance computing. Algorithmic Trading Infrastructure: Assist in developing execution algorithms, market connectivity modules, and risk management solutions. "}]