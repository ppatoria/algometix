[{"id":0,"href":"/blog/","title":"Blog: C++, Optimization, and Low-Latency Trading Systems","section":"Welcome to AlgoMetix","content":" Welcome to the Algometix Blog # This blog is dedicated to in-depth technical discussions on C++ programming, system optimization, and low-latency development for high-performance trading applications. Here, we explore:\nAdvanced C++ Concepts: Delve into under-the-hood mechanics of C++, covering templates, memory management, concurrency, and STL intricacies. Performance Optimization: Learn about cache efficiency, branch prediction, SIMD, and other techniques to fine-tune system performance. Low-Latency Trading Systems: Gain insights into market data handling, order book implementation, tick-to-trade optimization, and reducing latency bottlenecks. Algorithmic Trading Development: Explore order execution strategies, risk management algorithms, and infrastructure design for high-frequency trading. Expect highly technical, well-researched articles aimed at software engineers and quantitative developers looking to push system performance to the limits.\n"},{"id":1,"href":"/blog/multicore-caching/false-sharing/false_sharing_adjacent_variables/","title":"False Sharing: Impact of Adjacent Variable Modification by Multiple Threads","section":"False Sharing: Causes, Impact \u0026 Optimization Techniques","content":"False sharing occurs when multiple threads modify adjacent data stored in memory, leading to performance degradation due to unnecessary cache invalidations. In modern multi-core systems, where cache coherence protocols are crucial, minimizing false sharing becomes critical for optimizing performance in multithreaded applications.\nCache Line Contention: Visual Representation # Diagram for False Sharing in Adjacent Data # In this diagram, Variable A and Variable B are stored in the same cache line, causing both cores to continuously invalidate each other\u0026rsquo;s cache when either variable is updated.\nCode Examples: False Sharing in Independent and Struct Variables # 1. False Sharing with Independent Variables # #include \u0026lt;iostream\u0026gt; #include \u0026lt;thread\u0026gt; const int NUM_ITER = 10000000; int a = 0; // Modified by Thread 1 int b = 0; // Modified by Thread 2 void threadFunc1() { for (int i = 0; i \u0026lt; NUM_ITER; ++i) { a++; } } void threadFunc2() { for (int i = 0; i \u0026lt; NUM_ITER; ++i) { b++; } } int main() { std::thread t1(threadFunc1); std::thread t2(threadFunc2); t1.join(); t2.join(); std::cout \u0026lt;\u0026lt; \u0026#34;Final values: \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; b \u0026lt;\u0026lt; std::endl; } In this example, a and b are adjacent in memory, possibly on the same cache line. As thread 1 modifies a and thread 2 modifies b, the cache line containing both variables must be invalidated and reloaded, causing excessive cache coherence traffic and negatively affecting performance.\n2. False Sharing with Struct Variables # #include \u0026lt;iostream\u0026gt; #include \u0026lt;thread\u0026gt; const int NUM_THREADS = 2; const int NUM_ITER = 10000000; struct SharedData { int a; // Modified by Thread 1 int b; // Modified by Thread 2 } data; void threadFunc1() { for (int i = 0; i \u0026lt; NUM_ITER; ++i) { data.a++; // This will cause false sharing with data.b } } void threadFunc2() { for (int i = 0; i \u0026lt; NUM_ITER; ++i) { data.b++; // This will cause false sharing with data.a } } int main() { std::thread t1(threadFunc1); std::thread t2(threadFunc2); t1.join(); t2.join(); std::cout \u0026lt;\u0026lt; \u0026#34;Final values: \u0026#34; \u0026lt;\u0026lt; data.a \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; data.b \u0026lt;\u0026lt; std::endl; } Similarly, in this example, data.a and data.b are adjacent in memory, possibly on the same cache line. As thread 1 modifies data.a and thread 2 modifies data.b, the cache line containing both member variables must be invalidated and reloaded, causing excessive cache coherence traffic and negatively affecting performance.\nOptimizing for Performance: Mitigating False Sharing # 1. Use Alignment to Separate Variables # a. Stack or Global Variables with alignas # alignas(64) int a = 0; alignas(64) int b = 0; b. Heap Allocation with Alignment # int* a = static_cast\u0026lt;int*\u0026gt;(std::aligned_alloc(64, 64)); int* b = static_cast\u0026lt;int*\u0026gt;(std::aligned_alloc(64, 64)); Note: std::aligned_alloc is available only in C++17 and later versions. If working with an earlier C++ version, consider using posix_memalign or similar alternatives for heap alignment.\nPrevents automatic adjacent placement in memory.*\n2. Use Padding to Separate Variables # struct PaddedInt { int value; char padding[60]; // Assuming a 64-byte cache line }; PaddedInt a; PaddedInt b; struct alignas(64) SharedData { int a; char padding[60]; // Padding to ensure a and b do not share the same cache line int b; }; Forces a and b to be allocated in different cache lines, reducing contention.\n3. Using Thread-Local Storage (thread_local) # thread_local int a; thread_local int b; Using thread_local ensures that each thread gets its own instance of a and b, stored in thread-local storage, which prevents false sharing as the variables are not shared between threads.\nKey Takeaways # False sharing occurs when multiple threads modify adjacent variables that share the same cache line, leading to performance degradation. The problem is the same whether the variables are independent or part of a struct. Mitigation strategies include alignment, padding, splitting variables, and using thread-local storage. "},{"id":2,"href":"/blog/multicore-caching/","title":"Multi-Core Architecture \u0026 CPU Caching: Performance \u0026 Optimization","section":"Blog: C++, Optimization, and Low-Latency Trading Systems","content":" Multi-Core Caching Techniques # Introduction # Multi-core systems require specialized cache management strategies to maximize performance. In this section, we cover topics like cache coherence, false sharing, and how multi-core architecture impacts CPU caching.\nKey Topics: # Cache Coherence Protocols Cache Hierarchy and Sharing False Sharing in Multi-Core Systems Parallel Computing Optimization Explore the articles in this section to gain a deeper understanding of multi-core CPU caching and performance tuning.\nRelated Articles:\nCache Coherence Protocols Cache Hierarchy and Sharing in Multi-Processor Machines What is False Sharing? "},{"id":3,"href":"/blog/multicore-caching/false-sharing/what_is_false_sharing/","title":"False Sharing: A Hidden Performance Bottleneck in Multi-Core Systems","section":"False Sharing: Causes, Impact \u0026 Optimization Techniques","content":" What is False Sharing? # False sharing occurs when multiple threads modify independent variables that reside in the same cache line, leading to unnecessary cache invalidation and performance degradation. This issue is often subtle and may not be immediately apparent in code, but it can significantly impact performance in multi-threaded programs.\nWhy is it Called \u0026lsquo;False\u0026rsquo; Sharing? # What is shared? A cache line containing different variables used by multiple threads. Why \u0026lsquo;false\u0026rsquo;? The variables are not logically shared in terms of their use by different threads, but because they occupy the same cache line, any modification by one thread forces an update across multiple cores, causing inefficient cache invalidations and increased cache coherence traffic. When Does False Sharing Happen? # False sharing occurs in multi-threaded programs due to the interaction between the following factors:\nAdjacent variables: When threads modify independent variables stored adjacently in memory, they may end up in the same cache line. Cache coherence protocols: In systems with cache coherence protocols (e.g., MESI), even though each core has its own private cache (L1/L2), any modification in one core\u0026rsquo;s cache must be reflected across all cores to maintain consistency. This forces unnecessary cache invalidations and cache misses when a variable in the same cache line is modified by different threads. Private L1/L2 caches: Although caches are private to each core, the cache coherence protocol ensures consistency across caches, triggering updates or invalidations in other cores’ caches, which can degrade performance due to false sharing. This combination of adjacent data in the same cache line and the behavior of cache coherence protocols results in costly synchronization and performance degradation, even when the variables are independent.\n"},{"id":4,"href":"/blog/multicore-caching/cache_coherence_protocols/","title":"Cache Coherence and Protocols: Ensuring Data Consistency in Multi-Core Systems","section":"Multi-Core Architecture \u0026 CPU Caching: Performance \u0026 Optimization","content":" Introduction # Modern multi - core processors rely on private caches to reduce latency and improve performance.However, when multiple cores access the same memory location, ensuring consistency across caches becomes essential.**Cache coherence ** guarantees that all cores observe a consistent view of memory, preventing stale or incorrect data from affecting computations. This article explores why cache coherence is crucial, common problems that arise without it, and how protocols address these issues. Why Are Cache Coherence Protocols Important? # Cache coherence protocols ensure ** data consistency across multiple cores **by preventing stale reads, lost updates, and synchronization failures.Here are some critical problems that arise without cache coherence : ##1. Stale Reads(Reader - Writer Inconsistency) - **Problem : **One core writes a value while another core reads the same memory but sees an old(stale) value due to delayed cache updates. Example: A flag-based synchronization where the reader sees the flag but not the updated data. - Core 1 writes flag = true but the change is only in its private cache. - Core 2 checks flag, but it still sees the old value (false) and proceeds incorrectly. 2. Reordering in Producer-Consumer Patterns # Problem: A producer writes data and then sets a flag, but the consumer sees the flag before the data update propagates. Example: A message queue where msg.ready is observed before msg.payload is updated. 3. Broken Mutual Exclusion (Locks and Mutexes Failing) # Problem: A core locks a shared resource, but another core sees an outdated cached copy of the lock variable, causing multiple threads to enter a critical section. Example: A spinlock where both threads see lock = false due to stale cache. 4. Shared Data Structure Corruption (Linked Lists, Trees, Buffers) # Problem: One core modifies a pointer-based data structure while another core reads it, leading to segmentation faults or undefined behavior. Example: A linked list where a node is deleted, but another thread still has an outdated pointer. 5. Preventing Stale Data and Memory Visibility Issues # Each core has a private cache to minimize access time. Without synchronization, a core might read outdated (stale) data modified by another core.\nMemory Visibility and Ordering Issues # Stale Reads → A core sees an outdated value. Lost Updates → Multiple cores modify a variable, but some updates are lost. Synchronization Failures → Locks, mutexes, and atomic operations fail due to stale values. Reordering Issues → Dependent operations appear out of order. Example:\nCore 1 writes data = 42, then ready = 1, but Core 2 sees ready = 1 before data = 42, leading to incorrect execution. 6. Inconsistent Shared Data Structures # Problem: Pointers or references to shared data structures become stale or inconsistent due to memory updates not propagating correctly. Example: A node is deleted from a linked list, but another core still follows a stale pointer, leading to a segmentation fault. 7. Avoiding Data Races # Data races occur when multiple cores modify the same memory location simultaneously, leading to unpredictable behavior. Cache coherence protocols prevent such races by ensuring synchronized memory updates.\n8. Maintaining System Performance # Cache coherence protocols balance data consistency and performance overhead by reducing unnecessary cache invalidations and updates. This helps preserve cache efficiency while ensuring correctness.\n8.1 Avoiding Frequent Cache Misses # With Cache Coherence: Ensures cores can access recent updates without fetching from main memory. When a core reads a variable that was recently modified by another core, it may need to fetch the latest value from main memory (incurring a high latency). If cache coherence is absent, every read might result in a costly memory access. With Cache Coherence: Ensures cores can access recent updates without fetching from main memory. The protocol ensures that caches share the latest modified value efficiently. If a core needs data modified by another core, it can retrieve it from the other core\u0026rsquo;s cache instead of going to main memory. Performance Benefit: Reduces latency due to fewer cache misses. 8.2 Preventing Unnecessary Cache Invalidations # Optimized protocols like MESI reduce excessive invalidations, maintaining efficient cache usage. Without Optimization: Naïve invalidation policies may cause frequent invalidations, forcing cores to reload data from memory unnecessarily. With Optimized Cache Coherence: Modern protocols (e.g., MESI) use state-based tracking to only invalidate lines when absolutely necessary. Performance Benefit: Avoids excessive cache flushing, leading to better cache retention and reuse. 8.3 Reducing Bus Contention in Shared-Memory Systems # Without Cache Coherence: Multiple cores repeatedly fetch the same shared data from memory. This increases bus traffic, causing delays in other memory requests. With Cache Coherence: A modified cache line can be shared between cores without frequent memory access. Performance Benefit: Reduces memory bus congestion and improves overall execution speed. 8.4 Optimized Synchronization Performance # Without Cache Coherence: Locks and atomic operations require costly memory barriers. With Cache Coherence: Caches maintain coherence efficiently, ensuring that locks and synchronization primitives work with minimal overhead. Performance Benefit: Faster lock acquisition and release, improving multi-threaded application performance. Example: Ensuring Correct Locking with Cache Coherence # #include \u0026lt;atomic\u0026gt; #include \u0026lt;thread\u0026gt; #include \u0026lt;iostream\u0026gt; std::atomic\u0026lt;int\u0026gt; lock(0); void critical_section(int id) { while (lock.exchange(1, std::memory_order_acquire) == 1); // Critical section std::cout \u0026lt;\u0026lt; \u0026#34;Thread \u0026#34; \u0026lt;\u0026lt; id \u0026lt;\u0026lt; \u0026#34; entered critical section\\n\u0026#34;; lock.store(0, std::memory_order_release); } int main() { std::thread t1(critical_section, 1); std::thread t2(critical_section, 2); t1.join(); t2.join(); return 0; } This demonstrates how cache coherence ensures that all threads correctly see updates to the `lock** variable, preventing simultaneous access to the critical section.\nWithout cache coherence, Core 2 might still see lock = 0 even after Core 1 sets it to 1, leading to race conditions. Cache coherence ensures the updated value propagates correctly across cores, enabling efficient locking and synchronization.\nThe memory barriers (using memory_order_acquire and memory_order_release) ensure correct ordering of operations and enforce synchronization between cache and memory. However, cache coherence protocols play a separate but complementary role in this example.\n**Role of Memory Barriers vs. Cache Coherence in the Example** 1. **Memory Barriers (Acquire-Release Semantics)** - `memory_order_acquire` ensures that all prior writes from other threads become visible before proceeding. - `memory_order_release` ensures that all writes before releasing the lock are visible to other threads before the lock is set to 0. - This prevents instruction reordering and ensures correct synchronization. 2. **Cache Coherence Protocols (e.g., MESI)** - Even with memory barriers, cache coherence is required to ensure that updates to `lock` in **one core’s cache** are visible to other cores **without explicit memory flushes**. - If there were **no cache coherence**, Core 1 could write `lock = 1`, but Core 2 might still see `lock = 0` due to an outdated cache line. - **With cache coherence,** when Core 1 updates `lock = 1`, the protocol ensures Core 2 gets the updated value (by invalidating the stale copy or updating it directly). **What If There Was No Cache Coherence?** - Core 1 writes `lock = 1`, but Core 2 might still have a stale cached value (`lock = 0`). - This could lead to **two threads entering the critical section simultaneously**, breaking mutual exclusion. - Memory barriers alone do **not** force a cache update—they only control ordering. If there were no coherence mechanism, Core 2 would need **explicit memory flushes** (e.g., `std::atomic_thread_fence(std::memory_order_seq_cst)`) to synchronize data. **Memory Barriers vs. Cache Coherence** - **Memory barriers ensure ordering** of operations and prevent reordering. - **Cache coherence ensures visibility** of updates across cores **without explicit memory flushes**. - **Together,** they ensure correctness and improve performance by avoiding unnecessary memory operations. Conclusion # Cache coherence is a fundamental concept in multi-core systems, ensuring memory consistency, preventing stale reads, and optimizing synchronization mechanisms. By maintaining a coherent view of memory across cores, cache coherence protocols significantly enhance performance, correctness, and reliability in modern parallel computing environments.\n"},{"id":5,"href":"/blog/multicore-caching/cache-hierarchy-and-sharing-in-multi-processor-machines/","title":"Cache Hierarchy and Sharing in Multi-Processor Machines","section":"Multi-Core Architecture \u0026 CPU Caching: Performance \u0026 Optimization","content":" Typical Cache Hierarchy # In multi-core processors, the cache hierarchy plays a crucial role in improving performance by minimizing the time taken to access frequently used data. Here\u0026rsquo;s a breakdown of the typical cache levels:\nL1 Cache: The smallest and fastest cache, private to each core. L2 Cache: Larger than L1, usually private to each core but can also be shared in some systems. L3 Cache: The largest cache, shared across all cores in the processor. Main Memory (RAM): The largest and slowest storage layer, accessed when data is not found in the caches. Single-Core vs Multi-Core Architecture # We moved from single-core to multi-core processors, which made computers faster, especially by running tasks in parallel. However, this also increased complexity for developers, requiring careful optimization for efficient code execution. To understand this better, let’s examine the basic structure of multi-core processors and how they handle memory and cache.\nThe following diagram illustrates the difference between a single-core processor and a multi-core processor, highlighting the relationship between CPU cores, their respective caches, and main memory.\nUnderstanding the Diagram # The diagram shows two types of processors:\nSingle-Core Processor\nHas one core handling all tasks. Includes private L1 and L2 caches to store frequently used data. Accesses main memory (RAM) when data is not available in the cache. Multi-Core Processor\nHas multiple cores, allowing parallel execution. Each core has its own private L1 and L2 caches (as commonly seen in Intel processors). An L3 cache acts as a buffer for data exchange between cores and helps reduce main memory accesses. Reduces the need to access main memory, improving performance. Why This Matters # Multi-core processors improve speed and efficiency, but they also introduce new challenges like cache coherence, false sharing, and synchronization overhead. The rest of the article will explore these issues and their impact on performance.\nChecking Your CPU\u0026rsquo;s Cache Configuration # If you have an Intel processor like mine, you can check your system’s cache configuration using the following command:\n$ lscpu | grep -i cache L1d cache: 64 KiB (2 instances) L1i cache: 64 KiB (2 instances) L2 cache: 512 KiB (2 instances) L3 cache: 3 MiB (1 instance) Note: This configuration is specific to my Intel processor and may differ for other models.\nInterpretation # L1d \u0026amp; L1i caches: 64 KiB each with 2 instances → This indicates that L1 cache is private per core. L2 cache: 512 KiB with 2 instances → Each core likely has its own private L2 cache. L3 cache: 3 MiB with 1 instance → This shows that L3 cache is shared across all cores. This system has private L1 and L2 caches per core and a shared L3 cache, which is a common configuration in modern processors like those from Intel.\nCache Sharing and Potential Issues # Private Caches: Cores have their own L1 and L2 caches, designed to provide fast access to frequently used data specific to each core. Shared Cache: The L3 cache is shared by all cores, allowing for efficient communication and data exchange between cores. However, cache sharing introduces some challenges:\nIssues in Multi-Core Architectures # Cache Coherence: To ensure all cores have a consistent view of the data, cache coherence protocols (like MESI) are employed. These protocols track the state of each cache line to maintain consistency.\nFalse Sharing: False sharing occurs when independent variables, stored in the same cache line, are modified by different threads running on separate cores. This results in unnecessary cache invalidation, reducing performance.\nCache Thrashing: When multiple cores frequently access and modify the same cache line, it leads to constant invalidation and reloading, significantly impacting performance.\n"},{"id":6,"href":"/blog/cache-optimization/cache-optimization-through-prefetching/","title":"Cache Optimization through Prefetching: Enhancing Non-Contiguous Data Processing","section":"Cache Optimization: Boost CPU Performance \u0026 Reduce Latency","content":"In the realm of high-performance computing, cache optimization is crucial for ensuring efficient data processing. My previous article focused on optimizing cache usage through data alignment, where we rearranged data structures and leveraged container alignment to maximize cache utilization. This technique works well for data stored in contiguous memory. However, in real-world scenarios, data might not always be stored in contiguous blocks. This article focuses on how to handle or optimize such data for cache using prefetching techniques.\nUnderstanding Prefetching: # What is Prefetching? # Prefetching is a technique that tells the CPU to load data into the cache before it is needed. By doing this, we can reduce the time the CPU spends waiting for data from memory, which can significantly speed up processing.\nWhy Use Prefetching? # When data is not stored contiguously (e.g., linked lists), accessing it can cause cache misses. Prefetching helps avoid these misses by bringing the required data into the cache in advance.\nExample: Order Processing with Linked List # Let’s consider a simple example of processing Order data stored in a linked list:\nInitial Structure and Processing # struct Order { double price; int orderID; int quantity; }; Processing orders in a linked list without prefetching:\nvoid processOrdersWithoutPrefetching(std::list\u0026lt;Order\u0026gt;\u0026amp; orders) { for (const auto\u0026amp; order : orders) { processOrder(order); } } This straightforward approach may suffer from cache misses because linked lists store their elements in non-contiguous memory.\nIntroducing Prefetching # Prefetching instructs the CPU to load data into the cache before it is accessed, reducing memory access latency. Modern CPUs support prefetching with non-blocking instructions, allowing subsequent instructions to execute while prefetching occurs in parallel. This non-blocking nature ensures that the CPU can continue executing other instructions while the data is being fetched into the cache.\nEnhancing the above code with prefetching:\nvoid processOrdersWithPrefetching(std::list\u0026lt;Order\u0026gt;\u0026amp; orders) { auto it = orders.begin(); // Prefetch the first element if (it != orders.end()) { __builtin_prefetch(\u0026amp;(*it)); } for (; it != orders.end(); ++it) { auto nextIt = std::next(it); if (nextIt != orders.end()) { __builtin_prefetch(\u0026amp;(*nextIt)); } processOrder(*it); } } Note: In this article, we provide a general overview of prefetching and use the default parameters for __builtin_prefetch. Detailed explanations of the parameters and their tuning are beyond the scope of this article.\nKey Points About Prefetching # Non-Blocking Nature: # Prefetching does not stop the CPU from executing other instructions. While the data is being fetched, the CPU continues to process. Optimal Distance: # The CPU needs time to fetch the prefetched data into the cache. Prefetching too close to the current access point may not give enough time, causing delays. Prefetching too far ahead may waste resources. Step-by-Step Example # 1. **First Prefetch***: - At the start, we prefetch the first order before we enter the loop. - This ensures the first piece of data is ready to use right away. ```cpp auto it = orders.begin(); if (it != orders.end()) { __builtin_prefetch(\u0026amp;(*it)); } ``` 2. **Inside the Loop**: - Inside the loop, we keep prefetching the next order while processing the current one. - This keeps the next piece of data ready in advance. ```cpp for (; it != orders.end(); ++it) { auto nextIt = std::next(it); if (nextIt != orders.end()) { __builtin_prefetch(\u0026amp;(*it)); } processOrder(*it); } ``` 3. **Platform-Specific Instructions**: - `_mm_prefetch` works for Intel platforms. For other systems, check compiler-specific options, such as GCC’s `__builtin_prefetch`. Improved Prefetching with Batch Processing # Batch processing involves fetching and processing multiple data items together to reduce cache misses and improve performance.\nWhy Batch Processing? # Cache Line Utilization: Modern CPUs fetch data in blocks (cache lines). Using data in batches ensures efficient use of these blocks. Example Code for Batch Prefetching # Let’s fetch and process orders in batches of 4:\nvoid processOrdersInBatches(std::list\u0026lt;Order\u0026gt;\u0026amp; orders) { auto it = orders.begin(); Order* orderBatch[4]; while (it != orders.end()) { size_t batchSize = 0; // Prefetch the next 4 orders and collect them in a batch for (int i = 0; i \u0026lt; 4 \u0026amp;\u0026amp; it != orders.end(); ++i, ++it) { __builtin_prefetch(\u0026amp;(*it)); orderBatch[batchSize++] = \u0026amp;(*it); } // Process the batch of prefetched orders processOrderBatch(orderBatch, batchSize); } } Key Steps in Batch Processing # Why Size 4? # Cache Line Size: A typical cache line is 64 bytes. Order Size: Each Order struct is 16 bytes (after reordering). Fit 4 Orders: Since ( 64 , \\text{bytes} \\div 16 , \\text{bytes/order} = 4 ), we can fit 4 Order structs in one cache line. Step-by-Step Example # 1. **Initialize Batch and Iterator**: - We start by initializing an iterator and a batch array to hold 4 orders. ```cpp auto it = orders.begin(); Order* orderBatch[4]; ``` 2. **Prefetch and Process in Batches**: - Inside the loop, we prefetch the next 4 orders and store them in the batch array. - This ensures that the next 4 orders are fetched into the cache before they are processed. ```cpp while (it != orders.end()) { size_t batchSize = 0; // Prefetch the next 4 orders and collect them in a batch for (int i = 0; i \u0026lt; 4 \u0026amp;\u0026amp; it != orders.end(); ++i, ++it) { __builtin_prefetch(\u0026amp;(*it)); orderBatch[batchSize++] = \u0026amp;(*it); } // Process the batch of prefetched orders processOrderBatch(orderBatch, batchSize); } ``` 3. **Prefetch 4 Orders**: - The loop runs 4 times (if there are 4 orders available). - Each iteration prefetches one order and adds it to the batch array. 4. **Process the Batch**: - After collecting 4 orders (or fewer, if fewer are left), we process them in the batch. - This ensures that the fetched orders are used efficiently. Google Benchmark: # Run on (4 X 3300 MHz CPU s) CPU Caches: L1 Data 32 KiB (x2) L1 Instruction 32 KiB (x2) L2 Unified 256 KiB (x2) L3 Unified 3072 KiB (x1) Load Average: 3.07, 2.56, 1.98 ***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead. ----------------------------------------------------------------------------- Benchmark Time CPU Iterations ----------------------------------------------------------------------------- BM_ProcessOrdersWithoutPrefetching 5556028 ns 5548987 ns 254 BM_ProcessOrdersWithPrefetching 5335091 ns 5329073 ns 263 BM_ProcessOrdersWithPrefetchBatch 4569271 ns 4563944 ns 312 Explanation: # Compiler and Flags: The benchmark was built using GCC 13 with -O3 optimization. Observations: Prefetching showed a modest improvement over the baseline (WithoutPrefetching). The difference between WithoutPrefetching and WithPrefetching was small, likely due to compiler optimizations and automatic vectorization. Batch prefetching demonstrated the most significant performance gain due to efficient cache utilization. Conclusion # Prefetching is a powerful tool for optimizing cache usage, especially for non-contiguous data structures like linked lists. By understanding the non-blocking nature of prefetch instructions, combining them with batch processing, and tailoring them to specific hardware, you can significantly reduce cache misses and improve performance. This article complements the earlier discussion on data alignment, offering a comprehensive approach to cache optimization in diverse scenarios.\n"},{"id":7,"href":"/blog/cache-optimization/","title":"Cache Optimization: Boost CPU Performance \u0026 Reduce Latency","section":"Blog: C++, Optimization, and Low-Latency Trading Systems","content":" Cache Optimization Techniques # Introduction # Cache optimization plays a crucial role in boosting CPU performance, reducing latency, and improving overall system efficiency. In this section, we will explore advanced cache optimization strategies such as data alignment, prefetching, and more.\nKey Topics: # Data Alignment for Cache Efficiency Prefetching Techniques Minimizing Cache Misses Improving Data Locality Explore the articles and strategies in this section to enhance your understanding and implementation of cache optimization in real-time systems.\nRelated Articles:\nCache Optimization Focusing on Data Alignment Cache Optimization through Prefetching "},{"id":8,"href":"/blog/cache-optimization/cache-optimization-focusing-on-data-alignment/","title":"Cache Optimization: Focusing on Data Alignment","section":"Cache Optimization: Boost CPU Performance \u0026 Reduce Latency","content":" Introduction # Cache optimization is a cornerstone of high-performance computing. This article focuses on optimizing data alignment and data structures for improved cache utilization. By leveraging benchmarking results, we refine the design and implementation of a MarketData struct to illustrate tangible performance gains through alignment techniques.\nNote # This is the first article in a series on cache optimization, concentrating on alignment. To keep things simple, we omit multicore processing issues. In such scenarios, the shared nature of L1 and L2 caches introduces complexities like false sharing, fault tolerance, and more. These will be addressed in future articles. Techniques like buffering and prefetching, while omitted here, will also be covered in subsequent discussions.\nInitial Implementation: Baseline Design # The initial design of the MarketData struct is as follows:\nstruct MarketData { int symbol_id; // 4 bytes double price; // 8 bytes int volume; // 4 bytes }; Analysis # Field Sizes and Padding: int fields (symbol_id and volume) are 4 bytes each, while double price is 8 bytes. Due to alignment rules, the compiler adds padding after symbol_id (4 bytes) and after volume (4 bytes), making the total size of the struct 24 bytes (16 bytes for fields + 8 bytes padding). Cache Line Fit: Assuming a typical cache line size of 64 bytes, each instance of MarketData leaves unused space in the cache line. This could lead to inefficient cache utilization when processing arrays of MarketData. Let’s address these inefficiencies through iterative improvements.\nImproving Memory Layout and Alignment # Reordering Fields to Minimize Padding # By rearranging fields, we can reduce the padding and optimize the memory layout.\nstruct MarketDataReordered { double price; // 8 bytes int symbol_id; // 4 bytes int volume; // 4 bytes }; Explanation # Field Sizes and Padding: Placing the largest field (double price) first eliminates the padding after symbol_id. Total size is now 16 bytes (all fields fit contiguously without padding). Cache Line Fit: This smaller size increases the number of MarketDataReordered instances that fit in a single cache line, improving cache efficiency during sequential access. Explicit Alignment for Cache Line Optimization # Aligning the struct to the cache line size ensures that each instance starts at a cache-line boundary, reducing cache contention in multithreaded scenarios.\nstruct alignas(64) MarketDataAligned { double price; // 8 bytes int symbol_id; // 4 bytes int volume; // 4 bytes // Padding: 48 bytes (added to make total size 64 bytes) }; Note # While aligning to 64 bytes is useful in multithreaded contexts to avoid false sharing, it introduces unnecessary memory overhead for single-threaded applications. For optimal single-threaded performance, the MarketDataReordered struct (16 bytes) is preferred. Further Improvement by Aligning Containers to Cache Line # Efficient batch processing requires that arrays or vectors of data are also cache-aligned.\nUsing Aligned Arrays # Aligned arrays ensure contiguous, cache-friendly storage for fixed-size data.\ntemplate \u0026lt;typename T, std::size_t N\u0026gt; struct AlignedArray { alignas(64) std::array\u0026lt;T, N\u0026gt; data; }; using AlignedMarketDataArray = AlignedArray\u0026lt;MarketDataReordered, 1000\u0026gt;; Explanation # Field Sizes and Padding: Each MarketDataReordered instance is 16 bytes. The total size of the array is a multiple of 64 bytes, ensuring cache alignment. Cache Line Fit: Sequential processing of AlignedMarketDataArray leverages cache lines effectively, reducing cache misses. C-Style Aligned Arrays # For applications requiring C-style arrays, similar alignment can be achieved using explicit memory alignment.\nstruct AlignedCArray { alignas(64) MarketDataReordered data[1000]; }; Using Aligned Vectors # Dynamic arrays can also benefit from cache alignment by using a custom aligned allocator.\ntemplate \u0026lt;typename T, std::size_t Alignment\u0026gt; struct aligned_allocator { using value_type = T; T* allocate(std::size_t n) { void* ptr = nullptr; if (posix_memalign(\u0026amp;ptr, Alignment, n * sizeof(T)) != 0) throw std::bad_alloc(); return static_cast\u0026lt;T*\u0026gt;(ptr); } void deallocate(T* ptr, std::size_t) { free(ptr); } }; using AlignedVector = std::vector\u0026lt;MarketDataReordered, aligned_allocator\u0026lt;MarketDataReordered, 64\u0026gt;\u0026gt;; Explanation # Dynamic Size Flexibility: While dynamic allocation adds overhead, the aligned allocator ensures that data is cache-aligned for efficient access. Additional Optimizations # Loop Unrolling # Unrolling loops reduces loop control overhead and enhances instruction-level parallelism. Additionally, it leverages cache efficiency by processing multiple elements loaded into a cache line in a single iteration.\nvoid Process(AlignedMarketDataArray updates) { #pragma GCC unroll 4 for (const auto\u0026amp; update : updates.data) { // Process market data } } Why Unroll 4? # Cache Line Fit: With a MarketDataReordered size of 16 bytes and a 64-byte cache line, four instances fit perfectly into a single cache line. Unrolling the loop by 4 ensures that all elements loaded into the cache line are processed in one iteration, maximizing cache utilization. Reduced Loop Overhead: Fewer loop control instructions are executed, improving efficiency. Other Benefits: Unrolling also allows better instruction pipelining and parallelism, enabling the CPU to execute multiple instructions simultaneously. Explanation # By unrolling the loop to match the number of elements fitting into a cache line, we align the processing logic with hardware-level optimizations. This reduces memory access latencies and maximizes throughput. Conclusion # Through iterative optimizations, we achieved:\nReduced padding by reordering fields. Aligned data structures to cache line boundaries (where necessary). Leveraged aligned containers for batch processing. Enhanced performance with loop unrolling tailored to cache line size. Future work will explore techniques like buffering and prefetching, along with advanced considerations for multicore architectures, in subsequent articles.\n"},{"id":9,"href":"/about/","title":"About","section":"Welcome to AlgoMetix","content":" About AlgoMetix # Who Am I? # Welcome to AlgoMetix! I am a seasoned C++ developer\u0026hellip;\nAbout AlgoMetix # Who Am I? # Welcome to AlgoMetix! I am a seasoned C++ developer with over 20 years of experience in low-latency and algorithmic trading platforms. Having worked in front-office trading desks at leading investment banks like Credit Suisse and BNP Paribas, I have deep expertise in designing and optimizing real-world trading systems. My focus has always been on performance, efficiency, and robustness—critical aspects of any high-performance computing application.\nWhat is AlgoMetix? # AlgoMetix is inspired by the Sanskrit word Tattvartha, meaning \u0026ldquo;true essence of things.\u0026rdquo; This aligns with the website’s goal of exploring the fundamental principles behind high-performance computing, advanced C++ techniques, and real-world optimizations. The name reflects the focus on deep, practical insights rather than surface-level knowledge.\nWhat This Website Offers # This website is a place where I share my knowledge and experience in advanced C++ programming, low-latency system design, performance optimization, and real-world algorithmic trading architectures. My articles go beyond the standard programming books, covering:\nAdvanced C++ Techniques – Templates, memory management, multi-threading, lock-free data structures, and more. Low-Latency Optimization – CPU cache efficiency, false sharing, NUMA-aware programming, and compiler optimizations. Algorithmic Trading System Design – Order execution, market data handling, risk management, and connectivity with exchanges. Efficient and Optimized Code – Practical solutions that work in real-world production environments. Under-the-Hood Insights – How compilers, memory models, and CPU architectures impact software performance. Why AlgoMetix? # The internet is full of beginner-friendly programming tutorials, but real-world high-performance and low-latency system design is rarely covered in depth. My goal is to fill that gap by providing practical, production-grade insights that are immediately useful for professionals working in trading, high-performance computing, and systems programming.\nConsulting Services # With my extensive industry experience, I am also available for consulting on:\nPerformance tuning of existing C++ codebases. Designing and optimizing low-latency trading systems. Architecting scalable, high-performance applications. Code reviews, mentoring, and training teams on advanced C++ topics. Get in Touch # If you are interested in consulting, collaborations, or have any questions, feel free to contact me at ppatoria.jp@gmail.com. I am always open to discussions on high-performance programming, optimization, and real-world system design.\nThank you for visiting AlgoMetix. I hope you find the content valuable and insightful!\n"},{"id":10,"href":"/consulting/","title":"Consulting: High-Performance Trading System Development","section":"Welcome to AlgoMetix","content":" Consulting Services # I offer specialized consulting services in the domain of high-performance trading system development. With extensive experience in low-latency programming and real-time financial applications, I provide:\nSystem Architecture \u0026amp; Development: Design and build scalable, high-throughput trading platforms from scratch. Performance Optimization: Analyze and optimize C++ code for latency reduction, CPU efficiency, and cache performance. Code Reviews \u0026amp; Best Practices: Conduct deep technical reviews to ensure adherence to best practices in C++, concurrency, and high-performance computing. Algorithmic Trading Infrastructure: Assist in developing execution algorithms, market connectivity modules, and risk management solutions. "}]